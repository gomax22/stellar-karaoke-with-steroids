
Number of batches in train_dataloader: 1118
Number of batches in val_dataloader: 140
Number of batches in test_dataloader: 140
Device: cuda

Model loaded successfully frome epoch 11 - loss: 0.23481083219604834.
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
BetaVAE                                  [128, 1, 327680]          --
├─Sequential: 1-1                        [128, 256]                --
│    └─Conv1d: 2-1                       [128, 16, 163840]         128
│    └─LeakyReLU: 2-2                    [128, 16, 163840]         --
│    └─Conv1d: 2-3                       [128, 16, 81920]          1,296
│    └─LeakyReLU: 2-4                    [128, 16, 81920]          --
│    └─Conv1d: 2-5                       [128, 16, 40960]          1,296
│    └─LeakyReLU: 2-6                    [128, 16, 40960]          --
│    └─Conv1d: 2-7                       [128, 32, 20480]          1,568
│    └─LeakyReLU: 2-8                    [128, 32, 20480]          --
│    └─Conv1d: 2-9                       [128, 32, 10240]          3,104
│    └─LeakyReLU: 2-10                   [128, 32, 10240]          --
│    └─Conv1d: 2-11                      [128, 32, 5120]           3,104
│    └─LeakyReLU: 2-12                   [128, 32, 5120]           --
│    └─Conv1d: 2-13                      [128, 64, 2560]           6,208
│    └─LeakyReLU: 2-14                   [128, 64, 2560]           --
│    └─Conv1d: 2-15                      [128, 64, 1280]           12,352
│    └─LeakyReLU: 2-16                   [128, 64, 1280]           --
│    └─Conv1d: 2-17                      [128, 64, 640]            12,352
│    └─LeakyReLU: 2-18                   [128, 64, 640]            --
│    └─Conv1d: 2-19                      [128, 128, 320]           24,704
│    └─LeakyReLU: 2-20                   [128, 128, 320]           --
│    └─Conv1d: 2-21                      [128, 128, 160]           49,280
│    └─LeakyReLU: 2-22                   [128, 128, 160]           --
│    └─Conv1d: 2-23                      [128, 128, 80]            49,280
│    └─LeakyReLU: 2-24                   [128, 128, 80]            --
│    └─Conv1d: 2-25                      [128, 256, 40]            98,560
│    └─LeakyReLU: 2-26                   [128, 256, 40]            --
│    └─Conv1d: 2-27                      [128, 512, 20]            393,728
│    └─LeakyReLU: 2-28                   [128, 512, 20]            --
│    └─Conv1d: 2-29                      [128, 512, 20]            786,944
│    └─LeakyReLU: 2-30                   [128, 512, 20]            --
│    └─View: 2-31                        [128, 10240]              --
│    └─Linear: 2-32                      [128, 256]                2,621,696
├─Sequential: 1-2                        [128, 1, 327680]          --
│    └─Linear: 2-33                      [128, 10240]              1,320,960
│    └─View: 2-34                        [128, 512, 20]            --
│    └─ConvTranspose1d: 2-35             [128, 512, 20]            786,944
│    └─LeakyReLU: 2-36                   [128, 512, 20]            --
│    └─ConvTranspose1d: 2-37             [128, 256, 40]            524,544
│    └─LeakyReLU: 2-38                   [128, 256, 40]            --
│    └─ConvTranspose1d: 2-39             [128, 128, 80]            131,200
│    └─LeakyReLU: 2-40                   [128, 128, 80]            --
│    └─ConvTranspose1d: 2-41             [128, 128, 160]           65,664
│    └─LeakyReLU: 2-42                   [128, 128, 160]           --
│    └─ConvTranspose1d: 2-43             [128, 128, 320]           65,664
│    └─LeakyReLU: 2-44                   [128, 128, 320]           --
│    └─ConvTranspose1d: 2-45             [128, 64, 640]            32,832
│    └─LeakyReLU: 2-46                   [128, 64, 640]            --
│    └─ConvTranspose1d: 2-47             [128, 64, 1280]           16,448
│    └─LeakyReLU: 2-48                   [128, 64, 1280]           --
│    └─ConvTranspose1d: 2-49             [128, 64, 2560]           16,448
│    └─LeakyReLU: 2-50                   [128, 64, 2560]           --
│    └─ConvTranspose1d: 2-51             [128, 32, 5120]           8,224
│    └─LeakyReLU: 2-52                   [128, 32, 5120]           --
│    └─ConvTranspose1d: 2-53             [128, 32, 10240]          4,128
│    └─LeakyReLU: 2-54                   [128, 32, 10240]          --
│    └─ConvTranspose1d: 2-55             [128, 32, 20480]          4,128
│    └─LeakyReLU: 2-56                   [128, 32, 20480]          --
│    └─ConvTranspose1d: 2-57             [128, 16, 40960]          2,064
│    └─LeakyReLU: 2-58                   [128, 16, 40960]          --
│    └─ConvTranspose1d: 2-59             [128, 16, 81920]          1,040
│    └─LeakyReLU: 2-60                   [128, 16, 81920]          --
│    └─ConvTranspose1d: 2-61             [128, 16, 163840]         1,040
│    └─LeakyReLU: 2-62                   [128, 16, 163840]         --
│    └─ConvTranspose1d: 2-63             [128, 1, 327680]          65
│    └─LeakyReLU: 2-64                   [128, 1, 327680]          --
│    └─Conv1d: 2-65                      [128, 1, 327680]          4
==========================================================================================
Total params: 7,046,997
Trainable params: 7,046,997
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 133.81
==========================================================================================
Input size (MB): 167.77
Forward/backward pass size (MB): 13212.32
Params size (MB): 28.19
Estimated Total Size (MB): 13408.28
==========================================================================================
Epoch: 0 	| train_loss: 7.0390 	| train_recon_loss: 0.0430 	| train_kld_loss: 6.9959 	| val_loss: 1.5544 | val_recon_loss: 0.0479 | val_kld_loss: 1.5064
Epoch: 1 	| train_loss: 0.1049 	| train_recon_loss: 0.0462 	| train_kld_loss: 0.0586 	| val_loss: 1.5596 | val_recon_loss: 0.0433 | val_kld_loss: 1.5163
Epoch: 2 	| train_loss: 0.0840 	| train_recon_loss: 0.0445 	| train_kld_loss: 0.0395 	| val_loss: 1.2802 | val_recon_loss: 0.0424 | val_kld_loss: 1.2378
Epoch: 3 	| train_loss: 0.0732 	| train_recon_loss: 0.0436 	| train_kld_loss: 0.0296 	| val_loss: 1.3815 | val_recon_loss: 0.0411 | val_kld_loss: 1.3405
Epoch: 4 	| train_loss: 0.0806 	| train_recon_loss: 0.0427 	| train_kld_loss: 0.0379 	| val_loss: 1.4128 | val_recon_loss: 0.0404 | val_kld_loss: 1.3724
Epoch: 5 	| train_loss: 0.0818 	| train_recon_loss: 0.0422 	| train_kld_loss: 0.0396 	| val_loss: 1.8007 | val_recon_loss: 0.0403 | val_kld_loss: 1.7604
Epoch: 6 	| train_loss: 0.0930 	| train_recon_loss: 0.0423 	| train_kld_loss: 0.0507 	| val_loss: 1.5633 | val_recon_loss: 0.0400 | val_kld_loss: 1.5233
Epoch: 7 	| train_loss: 0.0990 	| train_recon_loss: 0.0424 	| train_kld_loss: 0.0566 	| val_loss: 0.9951 | val_recon_loss: 0.0401 | val_kld_loss: 0.9550
Epoch: 8 	| train_loss: 0.0940 	| train_recon_loss: 0.0424 	| train_kld_loss: 0.0516 	| val_loss: 0.4694 | val_recon_loss: 0.0402 | val_kld_loss: 0.4293
Epoch: 9 	| train_loss: 0.0850 	| train_recon_loss: 0.0423 	| train_kld_loss: 0.0427 	| val_loss: 0.4873 | val_recon_loss: 0.0400 | val_kld_loss: 0.4473
Epoch: 10 	| train_loss: 0.0839 	| train_recon_loss: 0.0418 	| train_kld_loss: 0.0420 	| val_loss: 0.2396 | val_recon_loss: 0.0392 | val_kld_loss: 0.2004
Epoch: 11 	| train_loss: 0.0794 	| train_recon_loss: 0.0414 	| train_kld_loss: 0.0380 	| val_loss: 0.3463 | val_recon_loss: 0.0394 | val_kld_loss: 0.3069
Epoch: 12 	| train_loss: 0.0757 	| train_recon_loss: 0.0412 	| train_kld_loss: 0.0345 	| val_loss: 0.2030 | val_recon_loss: 0.0390 | val_kld_loss: 0.1641
Epoch: 13 	| train_loss: 0.0758 	| train_recon_loss: 0.0410 	| train_kld_loss: 0.0348 	| val_loss: 0.2006 | val_recon_loss: 0.0386 | val_kld_loss: 0.1620
Epoch: 14 	| train_loss: 0.0732 	| train_recon_loss: 0.0407 	| train_kld_loss: 0.0325 	| val_loss: 0.2092 | val_recon_loss: 0.0386 | val_kld_loss: 0.1706
{'model_name': 'BetaVAE', 'model_loss': 0.2387513727215784, 'model_recon_loss': 0.039864445198327304, 'model_kld_loss': 0.19888692751826187}
Output shape: torch.Size([8944, 1, 327680])
